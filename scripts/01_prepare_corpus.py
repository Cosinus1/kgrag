# scripts/01_prepare_corpus.py

import sys
sys.path.append('.')

from src.preprocessing.document_loader import DocumentLoader
from src.preprocessing.text_cleaner import TextCleaner
from pathlib import Path

def main():
    # Configuration
    raw_dir = Path("data/raw")
    output_file = Path("data/processed/documents.json")
    
    print("="*60)
    print("PrÃ©paration du Corpus")
    print("="*60)
    
    # VÃ©rifier que le rÃ©pertoire existe
    if not raw_dir.exists():
        print(f"âŒ Erreur: Le rÃ©pertoire {raw_dir} n'existe pas!")
        print("   ExÃ©cutez d'abord: python scripts/00_load_data.py")
        return
    
    # Compter les fichiers
    txt_files = list(raw_dir.glob("*.txt"))
    print(f"\nğŸ“ RÃ©pertoire: {raw_dir.absolute()}")
    print(f"ğŸ“„ Fichiers trouvÃ©s: {len(txt_files)} fichiers .txt")
    
    if len(txt_files) == 0:
        print("\nâš ï¸  Aucun fichier trouvÃ©!")
        print("   ExÃ©cutez d'abord: python scripts/00_load_data.py")
        return
    
    # Charger les documents
    print(f"\n{'='*60}")
    print("Ã‰tape 1/3: Chargement des documents")
    print("="*60)
    
    loader = DocumentLoader(raw_dir)
    documents = loader.load_all_documents()
    
    print(f"\nâœ“ ChargÃ©: {len(documents)} documents")
    
    if len(documents) == 0:
        print("âŒ Aucun document chargÃ©! VÃ©rifiez les fichiers.")
        return
    
    # Nettoyer les textes
    print(f"\n{'='*60}")
    print("Ã‰tape 2/3: Nettoyage des textes")
    print("="*60)
    
    cleaner = TextCleaner()
    cleaned_count = 0
    
    for i, doc in enumerate(documents):
        try:
            original_length = len(doc['text'])
            doc['text'] = cleaner.clean(doc['text'])
            doc['paragraphs'] = cleaner.split_into_paragraphs(doc['text'])
            cleaned_length = len(doc['text'])
            
            # Statistiques de nettoyage
            reduction = 0
            if original_length > 0:
                reduction = ((original_length - cleaned_length) / original_length) * 100
            
            doc['cleaning_stats'] = {
                'original_length': original_length,
                'cleaned_length': cleaned_length,
                'reduction_percent': reduction,
                'num_paragraphs': len(doc['paragraphs'])
            }
            
            cleaned_count += 1
            
            if (i + 1) % 100 == 0:
                print(f"Progress: {i + 1}/{len(documents)} documents nettoyÃ©s...")
                
        except Exception as e:
            print(f"âš ï¸  Erreur lors du nettoyage du document {doc.get('filename', 'unknown')}: {e}")
            continue
    
    print(f"\nâœ“ NettoyÃ©: {cleaned_count}/{len(documents)} documents")
    
    # Sauvegarder
    print(f"\n{'='*60}")
    print("Ã‰tape 3/3: Sauvegarde")
    print("="*60)
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    loader.save_processed(documents, output_file)
    
    # Statistiques finales
    total_chars = sum(len(doc['text']) for doc in documents)
    total_paragraphs = sum(len(doc.get('paragraphs', [])) for doc in documents)
    
    print(f"\n{'='*60}")
    print("TERMINÃ‰!")
    print("="*60)
    print(f"âœ… Documents traitÃ©s: {len(documents)}")
    print(f"ğŸ“Š CaractÃ¨res totaux: {total_chars:,}")
    print(f"ğŸ“„ Paragraphes totaux: {total_paragraphs:,}")
    print(f"ğŸ’¾ SauvegardÃ© dans: {output_file.absolute()}")
    print(f"\nProchaine Ã©tape: python scripts/02_extract_entities.py")
    print("="*60)

if __name__ == "__main__":
    main()